The healthcare system today is heavily lacking technological modernization. As of January 1st 2019 hospitals and other medical facilities are lawfully obligated to make service and pricing information available to the public via something called a CMS or Charge Master. Unfortunately, the law still lacks in semantics, and these hospitals are not obligated to format this data in any specific manor. 

The purpose of my project is to develop a small part of the eventual prediction / learning model that will be utilized to standardize and offer an effective and quick search engine on these data sets, making information on pricing for medical care publicly available. The CMS Engine I have developed, takes in an array of these CMS data sets, and currently runs simplistic standardization on each of the description strings. It then processes and inserts each of these CMS data sets into individual tables in an SQL data base, which can be later read from and processed. The program by default searches through the “data” directory located within the project folder for the CSV files (CMS). The file “cms.py” containing the class “CMS2SQL” takes in one argument which is the name of the SQL data base that the data will be created if not exists and inserted into. 

My CMS Engine additionally has been programmed with capabilities to read from this SQL file, and runs string matching algorithms on the data sets. The string-matching algorithm that I chose to use for this process is Levenshtein distance. The file “processdb.py” containing the class “Dbdata” also conditionally takes in one argument which is the SQL file name or DB name. Currently the main of the program will prompt the user for an input string and will run the Levenshtein matching algorithm on each of the strings in the data base and return the closest matches along with the price of the procedures offered. The search query is stored into a dictionary, that can later be recalled from for future similar searches. The user can then press the keyboard interrupt CTRL-C to exit the program. 

The current standing program works very well for a data set that is so completely unstandardized for similar procedural descriptions. However, as it stands the program is unbelievably slow to perform the search, but this behavior was expected. For the future of the project, the idea would be to create a prediction model using machine learning / recursive algorithms to pre-process these data sets to resolve the acronyms / vague descriptions to a full named procedure by some degree of confidence. This will be done by getting a large array of medical dictionaries that are up to date with device names, pharmaceutical medications, and standard hospital procedures. I will likely also develop a web crawler that keeps these medical dictionaries up to data, and live feeds the information into the learning model. By doing this, the program can then pre-process these data sets against the list of medical terminology provided by these dictionaries. The descriptive strings can then be resolved to a standard i.e. {Chargemaster A : {“ins inj 10mg” : $20}, Chargemaster B: {“insln injcton 10mg” : $10}} would both resolve to -> insulin injection 10mg along with the corresponding price. After preprocessing this information, we can possibly use a Trie data structure along with appropriate ranking algorithms for a fully functional and effective healthcare search engine, with the preprocessed keys being the entirety of words in the medical dictionaries.


(i)	cms.py: contains class CMS2SQL(“dbname”). Dependencies: OS, CSV, string, sqlite3. 
Class functions:
a.	getData(): Gets the local data directory within the project folder. Data directory should contain CSV CMS files to read from. CSV CMS files contain columns description and price, each CSV representing different hospitals. Function walks through each of the CSV files and extracts the data from them storing in local class variable. Then calls the processCSV.
b.	processCSV(fileobj): Function takes in parameter object containing the individual data files location and naming convention. Function processes each of these by row and standardizes description strings and price for later read functionality. Function collects the data tuples from the CSV that will be written into the SQL. Calls write_to_sql
c.	write_to_sql(tbname, data): Function takes in the table name that will be inserted into the data base. Also takes in the data tuple for the chunk write into the data base. Executes file write into SQL data base using sqlite3. 
(ii)	processdb.py: contains class Dbdata(“dbname”). Dependencies: sqlite3, fuzzywuzzy, time. Class functions: 
a.	getAllTables(): Creates an SQL cursor to connect to the data base with passed name. Finds each of the tables and table names inside SQL DB and sets them into class variable. 
b.	extract_from_table(tbname): Takes in one parameter tbname, creates SQL connection and fetches the data from the table. Data chunks are appended to local variable for later processing.
c.	extractAll(): Iterates through table list in local variable and loops through each to call extract function.
d.	proto1sch(search string, limit=4, gettime=false): Conditional parameter search string, optional parameters limit for amount of search results, gettime defaults false, when made true will calculate processing time for debugging. Function uses Levenshtein Distance to assess most accurate string match in the data base. Returns the search results. 
e.	proto2sch(search string, limit=4, gettime=false): Explores a slightly quicker Levenshtein Distance process but results proven less accurate then previous method. Emulates same functionality as proto1sch by different method. Contains same optional and conditional parameters and returns same type of result object. 
f.	genwordlist(tup): Helper function for proto2sch, takes in data tuples and converts to array list of strings. 
(iii)	main.py: imports classes from above and runs each to generate SQL and import SQL for string match search function. Console program, displays results of search to screen with string description and price. 
